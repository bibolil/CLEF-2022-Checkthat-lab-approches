{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:02:59.39347Z","iopub.execute_input":"2022-04-20T23:02:59.394283Z","iopub.status.idle":"2022-04-20T23:02:59.407102Z","shell.execute_reply.started":"2022-04-20T23:02:59.394238Z","shell.execute_reply":"2022-04-20T23:02:59.406159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Arabic words cleaning","metadata":{}},{"cell_type":"markdown","source":"https://github.com/saobou/arabic-text-preprocessing/blob/master/Preprocess.ipynb","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/checkworthiness/checkworthiness/CT22_arabic_1A_checkworthy_train.tsv',sep='\\t')\ntest=pd.read_csv('/kaggle/input/checkworthiness/checkworthiness/CT22_arabic_1A_checkworthy_dev_test.tsv',sep='\\t')\ndev=pd.read_csv('/kaggle/input/checkworthiness/checkworthiness/CT22_arabic_1A_checkworthy_dev.tsv',sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:02:59.430096Z","iopub.execute_input":"2022-04-20T23:02:59.431075Z","iopub.status.idle":"2022-04-20T23:02:59.484324Z","shell.execute_reply.started":"2022-04-20T23:02:59.431025Z","shell.execute_reply":"2022-04-20T23:02:59.483405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyarabic\n!pip install farasapy\n!pip install tashaphyne","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-20T23:02:59.485964Z","iopub.execute_input":"2022-04-20T23:02:59.486228Z","iopub.status.idle":"2022-04-20T23:03:23.301693Z","shell.execute_reply.started":"2022-04-20T23:02:59.486198Z","shell.execute_reply":"2022-04-20T23:03:23.300675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom textblob import TextBlob\n\nfrom tashaphyne.stemming import ArabicLightStemmer\nfrom nltk.stem.isri import ISRIStemmer\nimport tashaphyne.arabic_const as arabconst \n\nstops = set(stopwords.words(\"arabic\"))\nstop_word_comp = {\"،\",\"آض\",\"آمينَ\",\"آه\",\"آهاً\",\"آي\",\"أ\",\"أب\",\"أجل\",\"أجمع\",\"أخ\",\"أخذ\",\"أصبح\",\"أضحى\",\"أقبل\",\"أقل\",\"أكثر\",\"ألا\",\"أم\",\"أما\",\"أمامك\",\"أمامكَ\",\"أمسى\",\"أمّا\",\"أن\",\"أنا\",\"أنت\",\"أنتم\",\"أنتما\",\"أنتن\",\"أنتِ\",\"أنشأ\",\"أنّى\",\"أو\",\"أوشك\",\"أولئك\",\"أولئكم\",\"أولاء\",\"أولالك\",\"أوّهْ\",\"أي\",\"أيا\",\"أين\",\"أينما\",\"أيّ\",\"أَنَّ\",\"أََيُّ\",\"أُفٍّ\",\"إذ\",\"إذا\",\"إذاً\",\"إذما\",\"إذن\",\"إلى\",\"إليكم\",\"إليكما\",\"إليكنّ\",\"إليكَ\",\"إلَيْكَ\",\"إلّا\",\"إمّا\",\"إن\",\"إنّما\",\"إي\",\"إياك\",\"إياكم\",\"إياكما\",\"إياكن\",\"إيانا\",\"إياه\",\"إياها\",\"إياهم\",\"إياهما\",\"إياهن\",\"إياي\",\"إيهٍ\",\"إِنَّ\",\"ا\",\"ابتدأ\",\"اثر\",\"اجل\",\"احد\",\"اخرى\",\"اخلولق\",\"اذا\",\"اربعة\",\"ارتدّ\",\"استحال\",\"اطار\",\"اعادة\",\"اعلنت\",\"اف\",\"اكثر\",\"اكد\",\"الألاء\",\"الألى\",\"الا\",\"الاخيرة\",\"الان\",\"الاول\",\"الاولى\",\"التى\",\"التي\",\"الثاني\",\"الثانية\",\"الذاتي\",\"الذى\",\"الذي\",\"الذين\",\"السابق\",\"الف\",\"اللائي\",\"اللاتي\",\"اللتان\",\"اللتيا\",\"اللتين\",\"اللذان\",\"اللذين\",\"اللواتي\",\"الماضي\",\"المقبل\",\"الوقت\",\"الى\",\"اليوم\",\"اما\",\"امام\",\"امس\",\"ان\",\"انبرى\",\"انقلب\",\"انه\",\"انها\",\"او\",\"اول\",\"اي\",\"ايار\",\"ايام\",\"ايضا\",\"ب\",\"بات\",\"باسم\",\"بان\",\"بخٍ\",\"برس\",\"بسبب\",\"بسّ\",\"بشكل\",\"بضع\",\"بطآن\",\"بعد\",\"بعض\",\"بك\",\"بكم\",\"بكما\",\"بكن\",\"بل\",\"بلى\",\"بما\",\"بماذا\",\"بمن\",\"بن\",\"بنا\",\"به\",\"بها\",\"بي\",\"بيد\",\"بين\",\"بَسْ\",\"بَلْهَ\",\"بِئْسَ\",\"تانِ\",\"تانِك\",\"تبدّل\",\"تجاه\",\"تحوّل\",\"تلقاء\",\"تلك\",\"تلكم\",\"تلكما\",\"تم\",\"تينك\",\"تَيْنِ\",\"تِه\",\"تِي\",\"ثلاثة\",\"ثم\",\"ثمّ\",\"ثمّة\",\"ثُمَّ\",\"جعل\",\"جلل\",\"جميع\",\"جير\",\"حار\",\"حاشا\",\"حاليا\",\"حاي\",\"حتى\",\"حرى\",\"حسب\",\"حم\",\"حوالى\",\"حول\",\"حيث\",\"حيثما\",\"حين\",\"حيَّ\",\"حَبَّذَا\",\"حَتَّى\",\"حَذارِ\",\"خلا\",\"خلال\",\"دون\",\"دونك\",\"ذا\",\"ذات\",\"ذاك\",\"ذانك\",\"ذانِ\",\"ذلك\",\"ذلكم\",\"ذلكما\",\"ذلكن\",\"ذو\",\"ذوا\",\"ذواتا\",\"ذواتي\",\"ذيت\",\"ذينك\",\"ذَيْنِ\",\"ذِه\",\"ذِي\",\"راح\",\"رجع\",\"رويدك\",\"ريث\",\"رُبَّ\",\"زيارة\",\"سبحان\",\"سرعان\",\"سنة\",\"سنوات\",\"سوف\",\"سوى\",\"سَاءَ\",\"سَاءَمَا\",\"شبه\",\"شخصا\",\"شرع\",\"شَتَّانَ\",\"صار\",\"صباح\",\"صفر\",\"صهٍ\",\"صهْ\",\"ضد\",\"ضمن\",\"طاق\",\"طالما\",\"طفق\",\"طَق\",\"ظلّ\",\"عاد\",\"عام\",\"عاما\",\"عامة\",\"عدا\",\"عدة\",\"عدد\",\"عدم\",\"عسى\",\"عشر\",\"عشرة\",\"علق\",\"على\",\"عليك\",\"عليه\",\"عليها\",\"علًّ\",\"عن\",\"عند\",\"عندما\",\"عوض\",\"عين\",\"عَدَسْ\",\"عَمَّا\",\"غدا\",\"غير\",\"ـ\",\"ف\",\"فان\",\"فلان\",\"فو\",\"فى\",\"في\",\"فيم\",\"فيما\",\"فيه\",\"فيها\",\"قال\",\"قام\",\"قبل\",\"قد\",\"قطّ\",\"قلما\",\"قوة\",\"كأنّما\",\"كأين\",\"كأيّ\",\"كأيّن\",\"كاد\",\"كان\",\"كانت\",\"كذا\",\"كذلك\",\"كرب\",\"كل\",\"كلا\",\"كلاهما\",\"كلتا\",\"كلم\",\"كليكما\",\"كليهما\",\"كلّما\",\"كلَّا\",\"كم\",\"كما\",\"كي\",\"كيت\",\"كيف\",\"كيفما\",\"كَأَنَّ\",\"كِخ\",\"لئن\",\"لا\",\"لات\",\"لاسيما\",\"لدن\",\"لدى\",\"لعمر\",\"لقاء\",\"لك\",\"لكم\",\"لكما\",\"لكن\",\"لكنَّما\",\"لكي\",\"لكيلا\",\"للامم\",\"لم\",\"لما\",\"لمّا\",\"لن\",\"لنا\",\"له\",\"لها\",\"لو\",\"لوكالة\",\"لولا\",\"لوما\",\"لي\",\"لَسْتَ\",\"لَسْتُ\",\"لَسْتُم\",\"لَسْتُمَا\",\"لَسْتُنَّ\",\"لَسْتِ\",\"لَسْنَ\",\"لَعَلَّ\",\"لَكِنَّ\",\"لَيْتَ\",\"لَيْسَ\",\"لَيْسَا\",\"لَيْسَتَا\",\"لَيْسَتْ\",\"لَيْسُوا\",\"لَِسْنَا\",\"ما\",\"ماانفك\",\"مابرح\",\"مادام\",\"ماذا\",\"مازال\",\"مافتئ\",\"مايو\",\"متى\",\"مثل\",\"مذ\",\"مساء\",\"مع\",\"معاذ\",\"مقابل\",\"مكانكم\",\"مكانكما\",\"مكانكنّ\",\"مكانَك\",\"مليار\",\"مليون\",\"مما\",\"ممن\",\"من\",\"منذ\",\"منها\",\"مه\",\"مهما\",\"مَنْ\",\"مِن\",\"نحن\",\"نحو\",\"نعم\",\"نفس\",\"نفسه\",\"نهاية\",\"نَخْ\",\"نِعِمّا\",\"نِعْمَ\",\"ها\",\"هاؤم\",\"هاكَ\",\"هاهنا\",\"هبّ\",\"هذا\",\"هذه\",\"هكذا\",\"هل\",\"هلمَّ\",\"هلّا\",\"هم\",\"هما\",\"هن\",\"هنا\",\"هناك\",\"هنالك\",\"هو\",\"هي\",\"هيا\",\"هيت\",\"هيّا\",\"هَؤلاء\",\"هَاتانِ\",\"هَاتَيْنِ\",\"هَاتِه\",\"هَاتِي\",\"هَجْ\",\"هَذا\",\"هَذانِ\",\"هَذَيْنِ\",\"هَذِه\",\"هَذِي\",\"هَيْهَاتَ\",\"و\",\"و6\",\"وا\",\"واحد\",\"واضاف\",\"واضافت\",\"واكد\",\"وان\",\"واهاً\",\"واوضح\",\"وراءَك\",\"وفي\",\"وقال\",\"وقالت\",\"وقد\",\"وقف\",\"وكان\",\"وكانت\",\"ولا\",\"ولم\",\"ومن\",\"مَن\",\"وهو\",\"وهي\",\"ويكأنّ\",\"وَيْ\",\"وُشْكَانََ\",\"يكون\",\"يمكن\",\"يوم\",\"ّأيّان\"}\nArListem = ArabicLightStemmer()\n\n\ndef stem(text):\n    zen = TextBlob(text)\n    words = zen.words\n    cleaned = list()\n    for w in words:\n        ArListem.light_stem(w)\n        cleaned.append(ArListem.get_root())\n    return \" \".join(cleaned)\n\nimport pyarabic.araby as araby\ndef normalizeArabic(text):\n    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n    text = re.sub(\"ى\", \"ي\", text)\n    text = re.sub(\"ؤ\", \"ء\", text)\n    text = re.sub(\"ئ\", \"ء\", text)\n    text = re.sub(\"ة\", \"ه\", text)\n    noise = re.compile(\"\"\" ّ    | # Tashdid\n                             َ    | # Fatha\n                             ً    | # Tanwin Fath\n                             ُ    | # Damma\n                             ٌ    | # Tanwin Damm\n                             ِ    | # Kasra\n                             ٍ    | # Tanwin Kasr\n                             ْ    | # Sukun\n                             ـ     # Tatwil/Kashida\n                         \"\"\", re.VERBOSE)\n    text = re.sub(noise, '', text)\n    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n    return araby.strip_tashkeel(text)\n\ndef remove_stop_words(text):\n    zen = TextBlob(text)\n    words = zen.words\n    return \" \".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.306115Z","iopub.execute_input":"2022-04-20T23:03:23.306563Z","iopub.status.idle":"2022-04-20T23:03:23.349645Z","shell.execute_reply.started":"2022-04-20T23:03:23.30651Z","shell.execute_reply":"2022-04-20T23:03:23.348594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trying examples","metadata":{}},{"cell_type":"code","source":"import re \ntext=\"قالوا الموفقُ شِيعِيٌّ فقلتُ لهم\"\nnormalizeArabic(text)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.35129Z","iopub.execute_input":"2022-04-20T23:03:23.351552Z","iopub.status.idle":"2022-04-20T23:03:23.35983Z","shell.execute_reply.started":"2022-04-20T23:03:23.351523Z","shell.execute_reply":"2022-04-20T23:03:23.358744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_dirt_after_transformation(text):\n    return re.sub('[a-z]|[A_Z]|[0-9]|\\n|:|N|,*', '', text)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.362185Z","iopub.execute_input":"2022-04-20T23:03:23.362685Z","iopub.status.idle":"2022-04-20T23:03:23.371861Z","shell.execute_reply.started":"2022-04-20T23:03:23.36264Z","shell.execute_reply":"2022-04-20T23:03:23.370787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.concat([train])\ntest_data=pd.concat([test,dev])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.373803Z","iopub.execute_input":"2022-04-20T23:03:23.374129Z","iopub.status.idle":"2022-04-20T23:03:23.38814Z","shell.execute_reply.started":"2022-04-20T23:03:23.374083Z","shell.execute_reply":"2022-04-20T23:03:23.387506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['tweet_text'] = data['tweet_text'].apply(lambda x:normalizeArabic(x))\ntest_data['tweet_text'] = test_data['tweet_text'].apply(lambda x:normalizeArabic(x))\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.38977Z","iopub.execute_input":"2022-04-20T23:03:23.39013Z","iopub.status.idle":"2022-04-20T23:03:23.66854Z","shell.execute_reply.started":"2022-04-20T23:03:23.390098Z","shell.execute_reply":"2022-04-20T23:03:23.667548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train = data.tweet_text.values\ny_train = data.class_label.values\n\nX_test = test_data.tweet_text.values\ny_test=test_data.class_label.values\n\nX_val=test_data.tweet_text.values\ny_val=test_data.class_label.values","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.670531Z","iopub.execute_input":"2022-04-20T23:03:23.671354Z","iopub.status.idle":"2022-04-20T23:03:23.678269Z","shell.execute_reply.started":"2022-04-20T23:03:23.671306Z","shell.execute_reply":"2022-04-20T23:03:23.677571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_accuracy(trained_model,X, y):\n    predicted = trained_model.predict(X)\n    accuracy = np.mean(predicted == y)\n    return accuracy\ndef get_F1(trained_model,X,y):\n    predicted=trained_model.predict(X)\n    f1=f1_score(y,predicted, average='binary')\n    return f1","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.679815Z","iopub.execute_input":"2022-04-20T23:03:23.680085Z","iopub.status.idle":"2022-04-20T23:03:23.692282Z","shell.execute_reply.started":"2022-04-20T23:03:23.680055Z","shell.execute_reply":"2022-04-20T23:03:23.691316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Deep Learning Approach**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport keras.models\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nimport gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.models import word2vec\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SimpleRNN\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,GlobalAveragePooling1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.69378Z","iopub.execute_input":"2022-04-20T23:03:23.694124Z","iopub.status.idle":"2022-04-20T23:03:23.709042Z","shell.execute_reply.started":"2022-04-20T23:03:23.69408Z","shell.execute_reply":"2022-04-20T23:03:23.707998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Tokenizer()function will split the sentence into tokens. The texts_to_sequences()convert word to integer number. ","metadata":{}},{"cell_type":"code","source":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(data[\"tweet_text\"])\nvocab_size = len(t.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.711795Z","iopub.execute_input":"2022-04-20T23:03:23.712597Z","iopub.status.idle":"2022-04-20T23:03:23.893758Z","shell.execute_reply.started":"2022-04-20T23:03:23.712556Z","shell.execute_reply":"2022-04-20T23:03:23.892111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_tokenized_train=t.texts_to_sequences(X_train)\nlist_tokenized_test=t.texts_to_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:23.895624Z","iopub.execute_input":"2022-04-20T23:03:23.895916Z","iopub.status.idle":"2022-04-20T23:03:24.046553Z","shell.execute_reply.started":"2022-04-20T23:03:23.895877Z","shell.execute_reply":"2022-04-20T23:03:24.0455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list_tokenized_train[:1] just for testing","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:24.048142Z","iopub.execute_input":"2022-04-20T23:03:24.048405Z","iopub.status.idle":"2022-04-20T23:03:24.053944Z","shell.execute_reply.started":"2022-04-20T23:03:24.048374Z","shell.execute_reply":"2022-04-20T23:03:24.052074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking word distribution to pick the max sentence length","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntotalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\nplt.hist(totalNumWords,bins = np.arange(0,600,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:24.05557Z","iopub.execute_input":"2022-04-20T23:03:24.055842Z","iopub.status.idle":"2022-04-20T23:03:24.396443Z","shell.execute_reply.started":"2022-04-20T23:03:24.055806Z","shell.execute_reply":"2022-04-20T23:03:24.395492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Most words have a length around 120 and all are less than 600 its a big number but better pick 580 to prevent any information loss","metadata":{}},{"cell_type":"code","source":"maxlen = 100\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:24.397691Z","iopub.execute_input":"2022-04-20T23:03:24.397916Z","iopub.status.idle":"2022-04-20T23:03:24.438648Z","shell.execute_reply.started":"2022-04-20T23:03:24.39789Z","shell.execute_reply":"2022-04-20T23:03:24.437721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LSTM with a pretrained word2vec model aravec**","metadata":{}},{"cell_type":"markdown","source":"loading the AraVec Skip-gram word embedding ","metadata":{}},{"cell_type":"code","source":"w2v_embeddings_index={}\nTOTAL_EMBEDDING_DIM=300\nembeddings_file='../input/aravec-documents/full_grams_sg_300_twitter/full_grams_sg_300_twitter.mdl'\nw2v_model =KeyedVectors.load(embeddings_file)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:03:24.440408Z","iopub.execute_input":"2022-04-20T23:03:24.44069Z","iopub.status.idle":"2022-04-20T23:04:09.549612Z","shell.execute_reply.started":"2022-04-20T23:03:24.44066Z","shell.execute_reply":"2022-04-20T23:04:09.548674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(w2v_model.wv.index_to_key)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:09.550973Z","iopub.execute_input":"2022-04-20T23:04:09.551765Z","iopub.status.idle":"2022-04-20T23:04:09.870356Z","shell.execute_reply.started":"2022-04-20T23:04:09.551715Z","shell.execute_reply":"2022-04-20T23:04:09.869624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_embeddings_index={}\nfor key in words:\n    w2v_embeddings_index[key] =w2v_model.wv.get_vector(key)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:09.871793Z","iopub.execute_input":"2022-04-20T23:04:09.872117Z","iopub.status.idle":"2022-04-20T23:04:14.663621Z","shell.execute_reply.started":"2022-04-20T23:04:09.872084Z","shell.execute_reply":"2022-04-20T23:04:14.662599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Loaded %s word vectors.\"% len(w2v_embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:14.665315Z","iopub.execute_input":"2022-04-20T23:04:14.665648Z","iopub.status.idle":"2022-04-20T23:04:14.672754Z","shell.execute_reply.started":"2022-04-20T23:04:14.665604Z","shell.execute_reply":"2022-04-20T23:04:14.671494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a weight matrix for words in training docs","metadata":{}},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, TOTAL_EMBEDDING_DIM))\nfor word, i in t.word_index.items():\n    embedding_vector = w2v_embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:14.674743Z","iopub.execute_input":"2022-04-20T23:04:14.675823Z","iopub.status.idle":"2022-04-20T23:04:14.858779Z","shell.execute_reply.started":"2022-04-20T23:04:14.675765Z","shell.execute_reply":"2022-04-20T23:04:14.857674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Embedding Matrix shape:\", embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:14.860165Z","iopub.execute_input":"2022-04-20T23:04:14.860905Z","iopub.status.idle":"2022-04-20T23:04:14.867783Z","shell.execute_reply.started":"2022-04-20T23:04:14.860851Z","shell.execute_reply":"2022-04-20T23:04:14.866226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The embedding layer is seeded by AraVec word embedding weight. The 300-dimensional Twitter Skip-gram version 3 was chosen. Therefore, the embedding layer defend with output_dim equal to 300.","metadata":{}},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size, TOTAL_EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlen, trainable=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:14.869632Z","iopub.execute_input":"2022-04-20T23:04:14.870002Z","iopub.status.idle":"2022-04-20T23:04:14.885218Z","shell.execute_reply.started":"2022-04-20T23:04:14.869957Z","shell.execute_reply":"2022-04-20T23:04:14.883442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lstm+aravec","metadata":{}},{"cell_type":"code","source":"input_placeholder= Input(shape=(maxlen,))\ninput_embedding = embedding_layer(input_placeholder)\nlstm= LSTM(100, return_sequences=True,name='lstm_layer_2')(input_embedding)\ny = GlobalAveragePooling1D()(lstm)\n#y = Dropout(0.1)(y)\ny = Dense(50, activation=\"relu\")(y)\ny = Dropout(0.1)(y)\npreds = Dense(1, activation='sigmoid', name = 'activation')(y)\nmodelaravec = Model(inputs=input_placeholder, outputs=preds)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:11:19.460375Z","iopub.execute_input":"2022-04-20T23:11:19.462879Z","iopub.status.idle":"2022-04-20T23:11:19.989869Z","shell.execute_reply.started":"2022-04-20T23:11:19.462828Z","shell.execute_reply":"2022-04-20T23:11:19.98894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelaravec.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:11:21.83421Z","iopub.execute_input":"2022-04-20T23:11:21.834592Z","iopub.status.idle":"2022-04-20T23:11:21.845933Z","shell.execute_reply.started":"2022-04-20T23:11:21.834555Z","shell.execute_reply":"2022-04-20T23:11:21.845177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#modelaravec.fit(X_t,y_train, batch_size=32, epochs=25, validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:04:15.536433Z","iopub.status.idle":"2022-04-20T23:04:15.537211Z","shell.execute_reply.started":"2022-04-20T23:04:15.536985Z","shell.execute_reply":"2022-04-20T23:04:15.537011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(modelaravec.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:11:23.653567Z","iopub.execute_input":"2022-04-20T23:11:23.654277Z","iopub.status.idle":"2022-04-20T23:11:23.664548Z","shell.execute_reply.started":"2022-04-20T23:11:23.654237Z","shell.execute_reply":"2022-04-20T23:11:23.663907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelaravec.fit(X_t,y_train, batch_size=32, epochs=10, validation_data=(X_te,y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:11:33.29325Z","iopub.execute_input":"2022-04-20T23:11:33.293618Z","iopub.status.idle":"2022-04-20T23:13:15.120192Z","shell.execute_reply.started":"2022-04-20T23:11:33.29358Z","shell.execute_reply":"2022-04-20T23:13:15.119335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = modelaravec.predict(X_te)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:13:19.094299Z","iopub.execute_input":"2022-04-20T23:13:19.095179Z","iopub.status.idle":"2022-04-20T23:13:20.588785Z","shell.execute_reply.started":"2022-04-20T23:13:19.095125Z","shell.execute_reply":"2022-04-20T23:13:20.587952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultat=[]\nfor i in range(len(result)):\n    resultat.append(result[i])","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:13:21.572837Z","iopub.execute_input":"2022-04-20T23:13:21.573693Z","iopub.status.idle":"2022-04-20T23:13:21.579739Z","shell.execute_reply.started":"2022-04-20T23:13:21.573654Z","shell.execute_reply":"2022-04-20T23:13:21.578708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predicted=[]\nfor i in range(len(resultat)):\n    if resultat[i]>0.5:\n        y_predicted.append(1)\n    else:\n        y_predicted.append(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:13:22.552988Z","iopub.execute_input":"2022-04-20T23:13:22.553342Z","iopub.status.idle":"2022-04-20T23:13:22.561417Z","shell.execute_reply.started":"2022-04-20T23:13:22.553309Z","shell.execute_reply":"2022-04-20T23:13:22.560417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss,accuracy = modelaravec.evaluate(X_te,y_test,verbose=0)\nprint(\"Accuracy: %5f\" % (accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:13:23.712893Z","iopub.execute_input":"2022-04-20T23:13:23.713807Z","iopub.status.idle":"2022-04-20T23:13:24.823269Z","shell.execute_reply.started":"2022-04-20T23:13:23.713765Z","shell.execute_reply":"2022-04-20T23:13:24.822205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\nbinf1=f1_score(y_test,y_predicted, average='binary')\nprint(\"F1-score binary :%5f\" % (binf1))","metadata":{"execution":{"iopub.status.busy":"2022-04-20T23:13:26.092798Z","iopub.execute_input":"2022-04-20T23:13:26.093371Z","iopub.status.idle":"2022-04-20T23:13:26.102409Z","shell.execute_reply.started":"2022-04-20T23:13:26.093331Z","shell.execute_reply":"2022-04-20T23:13:26.101429Z"},"trusted":true},"execution_count":null,"outputs":[]}]}